# @package system.optimizer_configs

- optimizer:
    _target_: torch.optim.AdamW
    lr: 1e-5 # use very small lr with CosineAnnealingWarmUpRestartsWithDecay
    # params: null # null or '' for all parameters
    # modules: null # null or '' for all module parameters
  lr_scheduler:
    scheduler:
      _target_: admet_prediction.modules.lr_scheduler.CosineAnnealingWarmUpRestartsWithDecay
      T_0: 10_000
      T_mult: null
      eta_max: 3e-4
      T_up: 2000
      gamma: 1.0
      warmup_base_lr: 0.0
    interval: "step"
    frequency: 1